{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Classification \n",
    "\n",
    "Problem: Identify the churning customer based on the features provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataset \n",
    "\n",
    "df = pd.read_csv(\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/ChurnData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 28 columns):\n",
      "tenure      200 non-null float64\n",
      "age         200 non-null float64\n",
      "address     200 non-null float64\n",
      "income      200 non-null float64\n",
      "ed          200 non-null float64\n",
      "employ      200 non-null float64\n",
      "equip       200 non-null float64\n",
      "callcard    200 non-null float64\n",
      "wireless    200 non-null float64\n",
      "longmon     200 non-null float64\n",
      "tollmon     200 non-null float64\n",
      "equipmon    200 non-null float64\n",
      "cardmon     200 non-null float64\n",
      "wiremon     200 non-null float64\n",
      "longten     200 non-null float64\n",
      "tollten     200 non-null float64\n",
      "cardten     200 non-null float64\n",
      "voice       200 non-null float64\n",
      "pager       200 non-null float64\n",
      "internet    200 non-null float64\n",
      "callwait    200 non-null float64\n",
      "confer      200 non-null float64\n",
      "ebill       200 non-null float64\n",
      "loglong     200 non-null float64\n",
      "logtoll     200 non-null float64\n",
      "lninc       200 non-null float64\n",
      "custcat     200 non-null float64\n",
      "churn       200 non-null float64\n",
      "dtypes: float64(28)\n",
      "memory usage: 43.8 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data preprocessing and Selection#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tenure   age  address  income   ed  employ  equip  callcard  wireless  \\\n",
      "0    11.0  33.0      7.0   136.0  5.0     5.0    0.0       1.0       1.0   \n",
      "1    33.0  33.0     12.0    33.0  2.0     0.0    0.0       0.0       0.0   \n",
      "2    23.0  30.0      9.0    30.0  1.0     2.0    0.0       0.0       0.0   \n",
      "\n",
      "   churn  \n",
      "0      1  \n",
      "1      1  \n",
      "2      0  \n",
      "Rows,Columns (200, 10)\n"
     ]
    }
   ],
   "source": [
    "#Select few features for study \n",
    "#Churn is label we would like to predict and convert it into int type  \n",
    "df = df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]\n",
    "df['churn'] = df['churn'].astype('int')\n",
    "\n",
    "print(df.head(3))\n",
    "\n",
    "print(\"Rows,Columns\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the feature callcard and wireless\n",
    "X = np.asarray(df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])\n",
    "y = np.asarray(df[['churn']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data set \n",
    "from sklearn import preprocessing\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.13518441, -0.62595491, -0.4588971 ,  0.4751423 ,  1.6961288 ,\n",
       "       -0.58477841, -0.85972695])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (160, 7) (160, 1)\n",
      "Test set: (40, 7) (40, 1)\n"
     ]
    }
   ],
   "source": [
    "#Split the data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "model = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#predict the test data \n",
    "\n",
    "test_label = model.predict(X_test)\n",
    "\n",
    "print(test_label)\n",
    "\n",
    "#probability of the test data prediction \n",
    "test_label_prob = model.predict_proba(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard_similarity_score: 0.75\n"
     ]
    }
   ],
   "source": [
    "#Model Evaluation method 1 #Jaccard_Similarity_Score\n",
    "\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "print(\"Jaccard_similarity_score:\",jaccard_similarity_score(y_test, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_loss: 0.602\n"
     ]
    }
   ],
   "source": [
    "#Model Evalution Log Loss Method \n",
    "from sklearn.metrics import log_loss\n",
    "print(\"log_loss:\",round(log_loss(y_test, test_label_prob),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "cnf_matrix = confusion_matrix(y_test,test_label, labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  9]\n",
      " [ 1 24]]\n"
     ]
    }
   ],
   "source": [
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.96      0.83        25\n",
      "           1       0.86      0.40      0.55        15\n",
      "\n",
      "   micro avg       0.75      0.75      0.75        40\n",
      "   macro avg       0.79      0.68      0.69        40\n",
      "weighted avg       0.78      0.75      0.72        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(y_test, test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the count of each section, we can calculate precision and recall of each label:\n",
    "\n",
    "Precision is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP / (TP + FP)\n",
    "\n",
    "Recall is true positive rate. It is defined as: Recall = TP / (TP + FN)\n",
    "\n",
    "So, we can calculate precision and recall of each class.\n",
    "\n",
    "F1 score: Now we are in the position to calculate the F1 scores for each label based on the precision and recall of that label.\n",
    "\n",
    "The F1score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.\n",
    "\n",
    "And finally, we can tell the average accuracy for this classifier is the average of the f1-score for both labels, which is 0.72 in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
